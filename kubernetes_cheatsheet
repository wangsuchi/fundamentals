kubernetes is one of the highest trending technology in cloud computing.
kubernetes has the fastest growth in job searching.Demand for engineers with knowledge on kubernetes is on rise

need everything to be container compatible.
⇒So we need Docker, or other equivalent container runtime engine installed on all the nodes in the cluster, 
including the master nodes, if you wish to host the controlling components as containers.

Use the code - DEVOPS15 - while registering for the CKA or CKAD exams at Linux Foundation to get a 15% discount.
Kubectl Reference Docs    search for imperative commands

1. ETCD
(1)what is ETCD?
  a distributed reliable key-value store, that is simple, secure & fast
  
(2)ETCDCTL is the CLI tool used to interact with ETCD.ETCDCTL can interact with ETCD Server using 2 API versions - Version 2 and Version 3.  
By default its set to use Version 2. Each version has different sets of commands.

For example ETCDCTL version 2 supports the following commands:
etcdctl backup
etcdctl cluster-health
etcdctl mk
etcdctl mkdir
etcdctl set
etcdctl get

Whereas the commands are different in version 3
etcdctl snapshot save 
etcdctl endpoint health
etcdctl put

To set the right version of API set the environment variable ETCDCTL_API command
export ETCDCTL_API=3

When API version is not set, it is assumed to be set to version 2. And version 3 commands listed above don't work. 
When API version is set to version 3, version 2 commands listed above don't work.

Apart from that, you must also specify path to certificate files so that ETCDCTL can authenticate to the ETCD API Server. 
The certificate files are available in the etcd-master at the following path. 
--cacert /etc/kubernetes/pki/etcd/ca.crt     
--cert /etc/kubernetes/pki/etcd/server.crt     
--key /etc/kubernetes/pki/etcd/server.key

2.Kube-API Server
Running kubectl command is in fact reaching to the kube-api server.
The kube-api server first authenticates the request and then validates it. It then retrieves data from etcd cluster and responds back.
Another way is to invoke APIs directly by sending a POST request. 

WHAT happened when you send a pod creating request to kube-api server?
When you ask apiserver to create a pod without signing it to a node, after finishing 3 steps above(authenicate use, validate request,retrieve data), 
it then update ETCD cluster.The scheduler notices that there's a new pod with no node assigned, so it identifies the right node to place the pod on and
communicates that back to kube-apiserver.The kube-apiserver then updates the information into etcd. The kube-apiserver then passes that information to the
kubelet in that appropiate worker node.The kubelet then creates the pod and instructs the container runtime engine to deploy the app image.Once done, kubelet
updates the status back to the API server, who will then updates the data back into etcd.

kubectl command
kubectl run nginx --image=nginx                           Create an NGINX Pod
kubectl run nginx --image=nginx --dry-run=client -o yaml  Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)
kubectl get pods                                          list of pods
kubectl describe pod [podName]                            get details of a pod
kubectl create/apply [filename.yaml]                      deploy pod by executing yaml file
kubectl apply command                                     Update the pod-definition file
kubectl edit replicaset/development  [rs/dev_name]        edit replicaset yaml file(change replicas permanently)
kubectl scale --replicas=3 rs/foo                         Scale a replicaset named 'foo' to 3
OR
kubectl scale --replicas=3  replicaset  [rs_name] 
kubectl create deployment --image=nginx nginx             Create a deployment
kubectl expose deployment nginx --port 80
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml                                     Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)
kubectl create -f nginx-deployment.yaml                   Save it to a file, make necessary changes to the file (for example, adding more replicas) and then create the deployment.
OR
kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx.yaml    specify the --replicas option to create a deployment with 4 replicas.

kubectl exec -it app -- cat /log/app.log  go into the app(podname), and run cat /log/app.log command in terminal

--dry-run: By default as soon as the command is run, the resource will be created. 
If you simply want to test your command , use --dry-run=client option. This will not create the resource, instead, tell you whether the resource can be created and if your command is right.

-o yaml: This will output the resource definition in YAML format on screen.

3.Kube Controller Manager-----node controller,replication controller,
A controller is a process that continuously monitors the state of various components within the system 
process name: kube-controller-manager

4.scheduler
IS only responsible for deciding which pod goes on which node

how to check whether you have scheduler or not?
kubectl get pods -n kube-system

To see the pod on which node
kubectl get pods -o wide

To check the taint of master node
kubectl describe node kubemaster | grep Taint

⇒taint(on node) (kubectl taint)& tolerations(on pod) (yaml)
node-target 
it tells specific node to only accepts certain pod with proper tolerations
However, taints and tolerations does not guarantee that the pods will only prefer these nodes.

⇒node selector(yaml:nodeSelector)
pod-target
you have to add label on node first by kubectl label,then edit yaml

⇒node affinity
Can't prevent other pods goes on the nodes.

Combination of taints & tolerations and node affinity can be used to completely dedicate nodes for specific pods

multiple schedulers--(1)how to customize which scheduler the pod will use
add schedulerName under spec column in pod yaml file
(2)how do u know which scheduler picked it up?
kubectl get events -o wide
(3)deploy additional scheduler as a pod(k8s document search for multiple scheduler)
my-scheduler-config.yaml

5.kubelet
captain of the worker node

6.kube proxy
Is a process that runs on each working node in k8s cluster.Its job is to look for new services.Everytime a new service is created, it creates the appropiate
rules on each node to forward traffic to those services to the backend pods.Like, iptable rule

7.Pod
With k8s, our ultimate aim is to deploy app in the form of containers on a set of machines that are configured as worker nodes in a cluster.
However,k8s does not deploy containers directly on the worker nodes.The containers are encapsulated into a k8s object known as pod.
A pod is a single instance of an app, which is the smallest object that one can created in k8s.

to overwrite entrypoint and CMD from image, use command for entrypoint, and args for CMD

spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "5000"
init container is run one at a time in sequential order
8.Replication Controller(deprecated)
It helps us run multiple instances of a single pod in k8s cluster, thus providing high availability.
Even if you have a single pod, the replication controller can automatically bring up a new pod when the existing one fails.
Another thing it can do is to share the load between pods, load balancing

What's replica set?　Almost the same as replication controller (new and recommended)
Replica Set can also manage pods that were not created as part of the Replica Set creation.
The selector is one of the major differences between Replication Controller and Replica Set.
The selector is not a required field in case of a Replication Controller but it is still available.
When you skip it as we did in the previous slide, it assumes it to be the same as the labels provided in the pod definition file.
In case of Replica Set, a user input is required for this property and it has to be written in the form of matched labels

9.service
type(1)NodePort
Mapping the port of the node to the port of the pod.
listen to a port and forward request to the Pods.
type(2)ClusterIP
the service creates a virtual IP inside the cluster to enable communication between different services,
such as a set of frontend servers to a set of backend servers.
type(3)loadBalancer
Exposes the Service externally using a cloud provider's load balancer

when the Pods are distributed across multiple nodes, we have the web application on Pods
on separate nodes in the cluster.When we create a service,
without us having to do any additional configuration,Kubernetes automatically creates a service that spans
across all the nodes in the cluster and maps the target port to the same node port on all the nodes in the cluster.
This way you can access your application using the IP of any node in the cluster and using the same port number

kubectl command to create a service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379(This will automatically use the pod's labels as selectors)

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml
(This will not use the pods labels as selectors, instead it will assume 
selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. 
So generate the file and modify the selectors before creating the service)

10.namespace
namespaces provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, 
but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc) 
and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc).

When you create a Service, it creates a corresponding DNS entry. This entry is of the form <service-name>.<namespace-name>.svc.cluster.local, 
which means that if a container only uses <service-name>, it will resolve to the service which is local to a namespace. This is useful for using 
the same configuration across multiple namespaces such as Development, Staging and Production. 

way to change ur default namespace?
kubectl config set-context $(kubectl config current-context) --namespace=dev

A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per namespace. It can limit the quantity of objects that can
be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that namespace.

11.daemon set
A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, 
Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. 
Deleting a DaemonSet will clean up the Pods it created.

12.static pods
Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. Unlike Pods that are managed by the 
control plane (for example, a Deployment); instead, the kubelet watches each static Pod (and restarts it if it fails).
Static Pods are always bound to one Kubelet on a specific node.
The kubelet automatically tries to create a mirror Pod on the Kubernetes API server for each static Pod.So you can see the pod in kubectl get pods 

(1)What is the path of the directory holding the static pod definition files?
Run the command ps -aux | grep kubelet 
and identify the config file - --config=/var/lib/kubelet/config.yaml. 
Then check in the config file for staticPodPath.

etcd,kube-apiserver,scheduler path: /etc/kubernetes/manifests
(2)delete static pod?
Identify which node the static pod is created on, ssh to the node(ssh nodename) and delete the pod definition file.

(3)way to create static pod
place file.yaml under the staticPodPath

13.logging
(1)need to plug in metrics service,then use kubectl command: kubectl top pod/ kubectl top node to see CPU/memory/disk consumption
(2)application logs: kubectl logs -f podname containername(if the pod only has one container, then you don't need to specify containername)

14. Rollout, versioning, Rolling updates, rollback
(1)rollout
When you first create a deployment, it triggers a rollout.A new rollout creates a new deployment revision, revison1.
In the future, when the app is upgraded,a new rollout is triggered and a new deployment revison is created, revision2.
This helps us keep track of the changes, enable us to roll back to previous revision if necessary.
see status of rollout:  kubectl rollout status deployment/[deploymentname]
see history of rollout:  kubectl rollout history deployment/[deploymentname]
(2)strategyType: recreate vs rolling update(default deployment strategy)
rolling update:
take down the older version and bring up a newer version one by one.This way the app never goes down, and the upgrade is seamless
(3)rollback
kubectl rollout undo deployment/[deploymentname]

15. configmaps
A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, 
command-line arguments, or as configuration files in a volume
step1: create configmap (imperative:kubectl create configmap/declarative:configfile,then kubectl create -f config-map.file)
step2: inject configuration data into the pod under yaml file(envFrom: configMapRef)

https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-configmap-em-
https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/

16. secret very like configmap
(envFrom: configMapRef)⇒(envFrom: secretRef)
imperative: kubectl create secret secretname --from-literal=key=value

17. take down node for os upgrade
Whenever a node goes offline, the master node waits for up to five minutes before considering the node dead.
The time it waits for a pod to come back online is known as the pod-eviction-timeout,and is set on the controller manager
with a default value of five minutes.

graceful way to take down node:
kubectl drain node-1(pods,managed by replicaset or daemonset, will be terminated,and deployed on other node instead.No new pods will be scheduled on this drained node during this time)
kubectl uncordon node-1(make node-1 schedule-able again for only new created pods)

relate command:
kubectl cordon node-1(make node-1 unschedule-able,doesn't move pods on it)
cat /etc/os-release (to check os,ubuntu or centos)

18 cluster upgrade process
kube-apiserver's version is the highest,say X
then controller-manager,kube-scheduler: X-1, 
kubelet:X-2,kube-proxy:X-2
kubectl: X+1 ~ X-1

kubeadm upgrade plan (What is the latest version available for an upgrade with the current version of the kubeadm tool installed?)
kubeadm upgrade apply

apt-get upgrade -y kubeadm=versionnum
apt-get upgrade -y kubelet=versionnum
kubeadm upgrade node config --kubelet-version versionnum
systemctl restart kubelet

19.security
19-1 authentication
methods: static password file,static token file(deprecate)
certificates,identity services

kubeadm environment:
where to look for the certificates:
cat /etc/kubernetes/manifests/kube-apiserver.yaml

manage cert with kubectl command in control manager:
kubectl get csr
kubectl certificate approve csr_name

to view the certificate:
kubectl get csr csr_name -o yaml
then decode certificate part to plain format by: echo "..." | base64 ---decode

to create csr.yaml
need to encode .csr into base 64 by command: cat akshay.csr | base64 -w 0(-w 0 is to print in single line)
 
 
 
 
 
 
 
 
 



